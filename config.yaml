seed: 1
epoch: 2
block_size: 0
block_decay: 00
optimizer: ksgld.KSGLD
accept_model: True
optim.SGD:
  lr: 1.e-1
optim.RMSprop:
  lr: 5.e-4
  weight_decay: 1.e-2
pysgmcmc_sgld.SGLD:
  lr: 5.e-4 # for pSGLD
  precondition_decay_rate: .95 # Exponential decay rate of the rescaling of the preconditioner (RMSprop)
  num_burn_in_steps: 10
  diagonal_bias: 1.e-8
sgld.SGLD:
  lr: 0.0005005742963403476 # for vanilla SGLD
  # lr: 99.e-2 # for pSGLD
  precondition_decay_rate: .95 # Exponential decay rate of the rescaling of the preconditioner (RMSprop)
  num_burn_in_steps: 610
  diagonal_bias: 1.e-8
  vanilla: True
sgld2.SGLD:
  lr: 0.5
  num_burn_in_steps: 300
sgld3.SGLD:
  lr: .1
  train_size: 60000
  lr_decay: 2.e-2
  weight_decay: 0.
  num_burn_in_steps: 300
psgld.pSGLD:
  lr: 1.e-2
  diagonal_bias: 5.e-5
  num_pseudo_batches: 1
  precondition_decay_rate: .95
  num_burn_in_steps: 300
psgld2.pSGLD:
  lr: 1.e-3
  train_size: 60000
  alpha: .95
  eps: 5.e-5
  num_burn_in_steps: 300
  centered: False
  addnoise: True
psgld3.pSGLD:
  lr: 1.e-3
  train_size: 60000
  lr_decay: 0.
  weight_decay: 0.
  num_burn_in_steps: 300
  rmsprop_decay: .99
  eps: 1.e-5
asgld.ASGLD:
  lr: 2.e-1
  momentum: .9
  weight_decay: 5.e-4
  eps: 5.e-5
  noise: 2.e-2
ksgld.KSGLD:
  lr: 0.1
  num_burn_in_steps: 300
  add_noise: True
  eps: 0.01   # Tikhonov regularization
  sua: False  # Applies SUA approximation.
  pi: False   # pi conrrection for Tikhonov regularization
  update_freq: 10
  alpha: 1.   # Running average parameter (if == 1, no r. ave.).
  constraint_norm: False  # Scale the gradients by the squared fisher norm.
eksgld.EKSGLD:
  eps: 0.01 # Tikhonov regularization
  sua: False
  ra: False # Computes stats using a running average of averaged gradients instead of using a intra minibatch estimate
  update_freq: 10
  alpha: 1. # Running average parameter (if == 1, no r. ave.).
dataset:
  train_batch: 100
  test_batch: 100