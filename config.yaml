seed: 1
epoch: 10
optimizer: optim.SGD
optim.SGD:
  lr: 1.e-1
optim.RMSprop:
  lr: 5.e-4
  weight_decay: 1.e-2
pysgmcmc_sgld.SGLD:
  lr: 5.e-4 # for pSGLD
  precondition_decay_rate: .95 # Exponential decay rate of the rescaling of the preconditioner (RMSprop)
  num_burn_in_steps: 10
  diagonal_bias: 1.e-8
sgld.SGLD:
  # lr: 5.e-4 # for vanilla SGLD
  lr: 99.e-2 # for pSGLD
  precondition_decay_rate: .95 # Exponential decay rate of the rescaling of the preconditioner (RMSprop)
  num_burn_in_steps: 10
  diagonal_bias: 1.e-8
  vanilla: False
asgld.ASGLD:
  momentum: .9
  weight_decay: 5.e-4
  eps: 1.e-6
  noise: 0.1
ksgld.KSGLD:
  eps: 0.01   # Tikhonov regularization
  sua: False  # Applies SUA approximation.
  pi: False   # pi conrrection for Tikhonov regularization
  update_freq: 10
  alpha: 1.   # Running average parameter (if == 1, no r. ave.).
  constraint_norm: False  # Scale the gradients by the squared fisher norm.
eksgld.EKSGLD:
  eps: 0.01 # Tikhonov regularization
  sua: False
  ra: False # Computes stats using a running average of averaged gradients instead of using a intra minibatch estimate
  update_freq: 10
  alpha: 1. # Running average parameter (if == 1, no r. ave.).
dataset:
  train_batch: 50
  test_batch: 50